{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "\n",
        "# üîß 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfLOr5Q8ZQfx",
        "outputId": "3172551d-4dad-407f-e00e-ef0c1c23b16a"
      },
      "outputs": [],
      "source": [
        "# Install all required Python packages for this workshop\n",
        "\n",
        "!pip install langchain langchain-community faiss-cpu pymupdf pypdf sentence_transformers rich wget python-dotenv cryptography langchain_ollama langchain-docling pymupdf4llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uqYW2IE5pvK"
      },
      "outputs": [],
      "source": [
        "import os, time\n",
        "from pathlib import Path\n",
        "\n",
        "import langchain\n",
        "import wget\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from langchain_docling import DoclingLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_docling.loader import ExportType\n",
        "import pymupdf4llm\n",
        "from langchain_core.documents.base import Document\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "console = Console()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPoWurPW_5gb"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gs5RIuCATIX"
      },
      "source": [
        "## 3. Extract Text from a Single PDF\n",
        "\n",
        "In this step, we‚Äôll load one PDF file and convert its pages into plain text (or Markdown) using three different methods:\n",
        "\n",
        "- **PyPDFLoader** (LangChain): A straightforward loader that splits the PDF into page-level `Document` objects.  \n",
        "- **PyMuPDF4LLM**: A fast, native extractor that generates Markdown-formatted text with optional page-wise chunking.  \n",
        "- **Docling**: A robust parser that preserves layout and exports content as Markdown, either per page (DOC_CHUNKS) or whole-document (MARKDOWN).\n",
        "\n",
        "You will see how to:\n",
        "\n",
        "1. Read the PDF from disk.  \n",
        "2. Extract every page‚Äôs text into a structured format.  \n",
        "3. Time each method to compare performance.  \n",
        "4. Preview a specific page for verification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsxUf8ZFCaiT"
      },
      "source": [
        "### üìÅ Setup Paths & Choose only 1 PDF for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "_V409t4BG0xw",
        "outputId": "599dc643-1d37-4f05-f735-d72bd1d29ebe"
      },
      "outputs": [],
      "source": [
        "# Create the \"data/sample_pdf\" folder if it doesn't exist\n",
        "SAMPLE_PDF_DIR = Path(\"data/sample_pdf\")\n",
        "os.makedirs(SAMPLE_PDF_DIR, exist_ok=True)\n",
        "\n",
        "# URL of the PDFs to test\n",
        "urls = [\n",
        "    \"https://github.com/ovaccarelli/LLM-RAG/blob/main/data/sample_pdf/2312.10997.pdf\"\n",
        "    \"https://github.com/ovaccarelli/LLM-RAG/blob/main/data/sample_pdf/2312.10997_page13.pdf\",\n",
        "]\n",
        "\n",
        "# Download the PDFs\n",
        "for url in urls:\n",
        "    name = url.split(\"/\")[-1]\n",
        "    if not (SAMPLE_PDF_DIR / name).is_file():\n",
        "        filename = wget.download(url, f\"data/PDFs/{name}\")\n",
        "console.print(\"Pdf file downloaded successfully.\", style=\"bold green\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyiWgsgf-7Mb"
      },
      "source": [
        "#### PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRr5M6QM-Ccx",
        "outputId": "980b6342-92fb-4e69-cebf-c293f1c71d77"
      },
      "outputs": [],
      "source": [
        "pdf_path = SAMPLE_PDF_DIR/\"2312.10997.pdf\"  # Just pick one page for testing\n",
        "\n",
        "# Load the PDF with PyPDFLoader\n",
        "start = time.time()\n",
        "loader = PyPDFLoader(str(pdf_path))\n",
        "docs_pypdf = loader.load()                 # returns a list of Document objects, one per page\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Using file: {pdf_path.name}\")\n",
        "print(f\"üïí PyPDFLoader loaded {len(docs_pypdf)} pages in {end - start:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r0rl1Qb7Icb",
        "outputId": "26c6b35c-654d-4dea-b593-dc7aebefb919"
      },
      "outputs": [],
      "source": [
        "# --- Preview the PDF contents ---\n",
        "# Pages are indexed starting from 0\n",
        "\n",
        "page_to_print = ...  # Change this to the page index you want\n",
        "max_num_characters = ... # Change the max num of characters you want to print\n",
        "\n",
        "# Now preview the chosen page:\n",
        "\n",
        "if 0 <= page_to_print < len(docs_pypdf):\n",
        "    content = docs_pypdf[page_to_print].page_content\n",
        "    print(f\"--- üìÑ Page {page_to_print + 1} / {len(docs_pypdf)} ---\\n\")\n",
        "    print(content[:max_num_characters])\n",
        "else:\n",
        "    print(f\"Page {page_to_print} is out of range (max:{len(docs_pypdf)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fg81Ftk4NqN"
      },
      "source": [
        "### PyMuPDF4LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGSv0GGPBteE",
        "outputId": "7a581948-2aa0-4e2c-8e8a-02c8854dec0c"
      },
      "outputs": [],
      "source": [
        "# Load the PDF with PyMuPDF4LLM\n",
        "start = time.time()\n",
        "docs_pymupdf = pymupdf4llm.to_markdown(str(pdf_path), page_chunks=True)       # return a list of page dicts\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Using file: {pdf_path.name}\")\n",
        "print(f\"üïí PyMuPDF4LLM extracted {len(docs_pymupdf)} pages in {end - start:.2f} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yzqls88v7QH0",
        "outputId": "3d3fa4bc-656f-4483-8c61-2653b5b2ea2a"
      },
      "outputs": [],
      "source": [
        "# --- Preview the PDF contents ---\n",
        "# Pages are indexed starting from 0\n",
        "\n",
        "page_to_print = ...  # Change this to the page index you want\n",
        "max_num_characters = ... # Change the max num of characters you want to print\n",
        "\n",
        "# Now preview the chosen page:\n",
        "\n",
        "if 0 <= page_to_print < len(docs_pymupdf):\n",
        "    md = docs_pymupdf[page_to_print][\"text\"]\n",
        "    print(f\"--- üìÑ Page {page_to_print + 1} / {len(docs_pymupdf)} ---\\n\")\n",
        "    print(md[:max_num_characters])\n",
        "else:\n",
        "    print(f\"Page {page_to_print} is out of range (max:{len(docs_pymupdf)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E-f6svc4TLP"
      },
      "source": [
        "### Docling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ79kroX_ygW",
        "outputId": "b3acdf59-a6a3-4f48-e9c8-4252f7199fa2"
      },
      "outputs": [],
      "source": [
        "pdf_path_docling = SAMPLE_PDF_DIR/\"2312.10997_page13.pdf\"  # Just pick one page for testing\n",
        "\n",
        "# Load the PDF with Docling\n",
        "start = time.time()\n",
        "loader_docling = DoclingLoader(str(pdf_path_docling), export_type=ExportType.MARKDOWN)\n",
        "docs_docling = loader_docling.load()\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Using file: {pdf_path_docling.name}\")\n",
        "print(f\"üïí Docling loaded {len(docs_docling)} document(s) in {end - start:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyZ74jEnBExT",
        "outputId": "78d164a3-b65e-4048-eb84-cf1eae168b80"
      },
      "outputs": [],
      "source": [
        "# --- Preview the PDF contents ---\n",
        "\n",
        "# Print the full extracted text\n",
        "for idx, doc in enumerate(docs_docling):\n",
        "    print(f\"\\n--- üìÑ PDF Document: {pdf_path_docling.name} ---\\n\")\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKgX3jOJ7OZT"
      },
      "source": [
        "## 4. Construct the vectorstore\n",
        "\n",
        "In this step, we take the PDF documents and transform them into a searchable vector database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the \"data/PDFs\" folder if it doesn't exist\n",
        "PDF_FOLDER = Path(\"data/PDFs\")\n",
        "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
        "\n",
        "urls = [\n",
        "    \"https://github.com/ovaccarelli/LLM-RAG/blob/main/data/PDFs/Open_Source_AI_workshop.pdf\",\n",
        "]\n",
        "\n",
        "# Download the PDFs\n",
        "for url in urls:\n",
        "    name = url.split(\"/\")[-1]\n",
        "    if not (PDF_FOLDER / name).is_file():\n",
        "        filename = wget.download(url, f\"data/PDFs/{name}\")\n",
        "console.print(\"Pdf file downloaded successfully.\", style=\"bold green\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0IUq-0eF1_x",
        "outputId": "babd1cf4-4e4e-492b-e52e-e72f0d2f8078"
      },
      "outputs": [],
      "source": [
        "# 1. Create a folder to store the vector index\n",
        "VECTORSTORES_DIR = Path(\"data/vectorstores\")\n",
        "os.makedirs(VECTORSTORES_DIR, exist_ok=True)\n",
        "\n",
        "# 2. Point to the directory containing our PDFs\n",
        "PDF_FOLDER = Path(\"data/PDFs\")\n",
        "\n",
        "# 3. Use PyPDFDirectoryLoader to load every PDF page as a Document\n",
        "loader = PyPDFDirectoryLoader(PDF_FOLDER)\n",
        "documents = loader.load()\n",
        "\n",
        "# 4. Verify how many pages are loaded\n",
        "print(f\"Loaded {len(documents)} PDF pages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69xtJTpC0uk"
      },
      "source": [
        "### ‚úÇÔ∏è Split Documents into Chunks\n",
        "\n",
        "We break documents into smaller overlapping chunks using `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "- `chunk_size`: The number of characters per chunk.\n",
        "\n",
        "- `chunk_overlap`: Ensures that we maintain context between chunks.\n",
        "\n",
        "This is crucial for preserving semantic meaning across sentences and paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u2AIMgmMNHw",
        "outputId": "df3f34f8-2aaf-497f-ec9e-8e87df2db812"
      },
      "outputs": [],
      "source": [
        "# Set chunk size (how many characters per chunk) and overlap\n",
        "CHUNK_SIZE = ...\n",
        "CHUNK_OVERLAP = ...\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Split the loaded PDFs into smaller, overlapping chunks\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"‚úÖ Split into {len(all_splits)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHjxcY36DCc_"
      },
      "source": [
        "### üîç Convert Text Chunks to Embeddings\n",
        "\n",
        "We now convert each text chunk into a high-dimensional vector using the BGE model (`BAAI/bge-large-en-v1.5`). These vectors capture the semantic meaning of the text.\n",
        "\n",
        "- We use `HuggingFaceBgeEmbeddings from LangChain`.\n",
        "\n",
        "- Normalizing embeddings helps improve similarity search accuracy.\n",
        "\n",
        "- We set the device to \"cpu\" for compatibility with Colab. (If you're running this on a local machine with GPU, you can switch \"cpu\" to \"cuda\" for better performance.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0UZzf5GMQ0c"
      },
      "outputs": [],
      "source": [
        "# Define the embedding model ‚Äî BGE is a strong open-source embedding model for English\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
        "\n",
        "embedding_model = HuggingFaceBgeEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs={\"device\": \"cpu\"},  # \"cuda\" if you run locally with a GPU\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDIcoz_DWI4"
      },
      "source": [
        "### üèóÔ∏è Create and Save the Vectorstore\n",
        "\n",
        "Using the text chunks and embeddings, we build our vectorstore:\n",
        "\n",
        "- FAISS (Facebook AI Similarity Search) is a fast library for vector similarity search.\n",
        "\n",
        "- This index will let us retrieve the most relevant chunks given a user question.\n",
        "\n",
        "We also save the vectorstore locally so that it can be reused later without recomputing everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4XZVZMpNKmC",
        "outputId": "21c28fdb-bab9-4cb6-e792-9c7c5e14affe"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS index from the text chunks and their embeddings\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n",
        "\n",
        "# Save the vectorstore locally for reuse\n",
        "vectorstore.save_local(VECTORSTORES_DIR)\n",
        "\n",
        "print(\"‚úÖ Vectorstore created and saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n5XOWboD24O"
      },
      "source": [
        "üíæ Reload the Vectorstore (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whi6g_8TEEoJ",
        "outputId": "e183a27d-d437-49f3-fd0e-19fea68185c3"
      },
      "outputs": [],
      "source": [
        "# You can reload the saved vectorstore anytime without recomputing everything\n",
        "vectorstore = FAISS.load_local(\n",
        "    VECTORSTORES_DIR,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True  # Required in Colab environments\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Vectorstore reloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze4WmNqKD-Zd"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
